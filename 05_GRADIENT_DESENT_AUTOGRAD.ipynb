{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a617e3",
   "metadata": {},
   "source": [
    "### ìë™ ë¯¸ë¶„ (torch.autograd)\n",
    "\n",
    "- PyTorchì˜ autograd ëŠ” ì‹ ê²½ë§ í›ˆë ¨ì„ ì§€ì›í•˜ëŠ” ìë™ ë¯¸ë¶„ ê°€ëŠ¥\n",
    "- torch.autograd ë™ì‘ ë°©ë²•\n",
    "    - í…ì„œì— .requires_grad ì†ì„±ì„ Trueë¡œ ì„¤ì •í•˜ë©´, ì´í›„ì˜ í…ì„œ ëª¨ë“  ì—°ì‚°ë“¤ì„ ì¶”ì í•¨\n",
    "    - í…ì„œ.backward() ë¥¼ í˜¸ì¶œí•˜ë©´, ì—°ì‚°ì— ì—°ê²°ëœ ê° í…ì„œë“¤ì˜ ë¯¸ë¶„ ê°’ì„ ê³„ì‚°í•˜ì—¬, ê° í…ì„œ ê°ì²´ì— .gradì— ì €ì¥\n",
    "        - .requires_grad_()ëŠ” ì—°ê²°ëœ Tensorë¡œë¶€í„°ì˜ ê³„ì‚°ëœ ìë™ë¯¸ë¶„ ê°’ì„, ë‹¤ì‹œ í˜„ í…ì„œë¶€í„° ì‹œì‘í•˜ë„ë¡ ë§Œë“¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5044f",
   "metadata": {},
   "source": [
    "### ì‹ ê²½ë§ ë™ì‘ ì´í•´\n",
    "\n",
    "- ëª¨ë¸ ë° ë°ì´í„° ìƒì„±\n",
    "- forward passë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ì„œ ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
    "- ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ loss function ìœ¼ë¡œ ê³„ì‚°\n",
    "- backward pass ë¡œ ê° ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ loss ê°’ ê¸°ë°˜ ë¯¸ë¶„í•˜ì—¬ ì €ì¥\n",
    "- optimizer ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´, íŒŒë¼ë¯¸í„° ê°’ ì—…ë°ì´íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe383b7",
   "metadata": {},
   "source": [
    "- í…ì„œì— .requires_grad ì†ì„±ì„ True ë¡œ ì„¤ì •\n",
    "- .requires_grad ì†ì„±ì´ True ë¡œ ì„¤ì •ë˜ë©´, í…ì„œì˜ ëª¨ë“  ì—°ì‚° ì¶”ì ì„ ìœ„í•´, ë‚´ë¶€ì ìœ¼ë¡œ ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG : Directed Acyclic Graph)ë¥¼ ë™ì  êµ¬ì„±\n",
    "    - ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG)ì˜ leaf ë…¸ë“œëŠ” ì…ë ¥ í…ì„œì´ê³ , root ë…¸ë“œëŠ” ê²°ê³¼ í…ì„œê°€ ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342f3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "y = torch.rand(1)\n",
    "y.requires_grad = True\n",
    "loss = y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfe2a7",
   "metadata": {},
   "source": [
    "### ğŸ” í…ì„œì˜ `.backward()` ë™ì‘ ì„¤ëª…\n",
    "\n",
    "- `tensor.backward()` ë¥¼ í˜¸ì¶œí•˜ë©´,  \n",
    "  ì—°ì‚°ì— ì—°ê²°ëœ ê° í…ì„œë“¤ì˜ **ë¯¸ë¶„ ê°’(gradient)**ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬  \n",
    "  ê° í…ì„œ ê°ì²´ì˜ `.grad` ì†ì„±ì— ì €ì¥ëœë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ ì˜ˆì‹œ\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial x} = -1, \\quad \\frac{\\partial \\text{Loss}}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "> ì¦‰, Lossë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ì…ë ¥ê°’ì— ëŒ€í•œ **ê¸°ìš¸ê¸°(gradient)**ê°€ ê³„ì‚°ë˜ì–´  \n",
    "> `.grad` ì†ì„±ì— ìë™ìœ¼ë¡œ ì €ì¥ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd5c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2e3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0033, 0.4749, 0.2218],\n",
      "        [0.8056, 0.3840, 0.0867],\n",
      "        [0.5820, 0.8906, 0.5938],\n",
      "        [0.6829, 0.3664, 0.6069]], requires_grad=True) tensor([0.5548, 0.9362, 0.5315], requires_grad=True) tensor([2.6285, 3.0521, 2.0408], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x,W) + b\n",
    "print(W, b, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949683ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7964, grad_fn=<MseLossBackward0>) tensor([[1.7524, 2.0347, 1.3605],\n",
      "        [1.7524, 2.0347, 1.3605],\n",
      "        [1.7524, 2.0347, 1.3605],\n",
      "        [1.7524, 2.0347, 1.3605]]) tensor([1.7524, 2.0347, 1.3605])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(6.7964, grad_fn=<MseLossBackward0>) tensor([2.6285, 3.0521, 2.0408], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "2 tensor(3.0206, grad_fn=<MseLossBackward0>) tensor([1.7524, 2.0347, 1.3605], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "3 tensor(1.3425, grad_fn=<MseLossBackward0>) tensor([1.1682, 1.3565, 0.9070], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "4 tensor(0.5967, grad_fn=<MseLossBackward0>) tensor([0.7788, 0.9043, 0.6047], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "5 tensor(0.2652, grad_fn=<MseLossBackward0>) tensor([0.5192, 0.6029, 0.4031], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "6 tensor(0.1179, grad_fn=<MseLossBackward0>) tensor([0.3461, 0.4019, 0.2687], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "7 tensor(0.0524, grad_fn=<MseLossBackward0>) tensor([0.2308, 0.2679, 0.1792], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.1\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold :\n",
    "    iteration_num += 1\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "    \n",
    "    # detach_() : í…ì„œë¥¼ ê¸°ì¡´ ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG : Directed Acyclid Graph) ë¡œë¶€í„° ëŠìŒ\n",
    "    # .requires_grad(True) : ì—°ê²°ëœ Tensor ë¡œë¶€í„°ì˜ ê³„ì‚°ëœ ìë™ë¯¸ë¶„ ê°’ì„, ë‹¤ì‹œ í˜„ í…ì„œë¶€í„° ì‹œì‘í•˜ë„ë¡ ë§Œë“¦\n",
    "    W.detach_().requires_grad_(True)\n",
    "    b.detach_().requires_grad_(True)\n",
    "\n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "\n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def7d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c87773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54fb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa92106b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
