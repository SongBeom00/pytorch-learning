{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a617e3",
   "metadata": {},
   "source": [
    "### 자동 미분 (torch.autograd)\n",
    "\n",
    "- PyTorch의 autograd 는 신경망 훈련을 지원하는 자동 미분 가능\n",
    "- torch.autograd 동작 방법\n",
    "    - 텐서에 .requires_grad 속성을 True로 설정하면, 이후의 텐서 모든 연산들을 추적함\n",
    "    - 텐서.backward() 를 호출하면, 연산에 연결된 각 텐서들의 미분 값을 계산하여, 각 텐서 객체에 .grad에 저장\n",
    "        - .requires_grad_()는 연결된 Tensor로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5044f",
   "metadata": {},
   "source": [
    "### 신경망 동작 이해\n",
    "\n",
    "- 모델 및 데이터 생성\n",
    "- forward pass로 입력 데이터를 모델에 넣어서 예측값 계산\n",
    "- 예측값과 실제값의 차이를 loss function 으로 계산\n",
    "- backward pass 로 각 모델 파라미터를 loss 값 기반 미분하여 저장\n",
    "- optimizer 로 모델 파라미터의 최적값을 찾기 위해, 파라미터 값 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe383b7",
   "metadata": {},
   "source": [
    "- 텐서에 .requires_grad 속성을 True 로 설정\n",
    "- .requires_grad 속성이 True 로 설정되면, 텐서의 모든 연산 추적을 위해, 내부적으로 방향성 비순환 그래프(DAG : Directed Acyclic Graph)를 동적 구성\n",
    "    - 방향성 비순환 그래프(DAG)의 leaf 노드는 입력 텐서이고, root 노드는 결과 텐서가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342f3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "y = torch.rand(1)\n",
    "y.requires_grad = True\n",
    "loss = y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfe2a7",
   "metadata": {},
   "source": [
    "### 🔁 텐서의 `.backward()` 동작 설명\n",
    "\n",
    "- `tensor.backward()` 를 호출하면,  \n",
    "  연산에 연결된 각 텐서들의 **미분 값(gradient)**을 자동으로 계산하여  \n",
    "  각 텐서 객체의 `.grad` 속성에 저장된다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 예시\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial x} = -1, \\quad \\frac{\\partial \\text{Loss}}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "> 즉, Loss를 기준으로 각 입력값에 대한 **기울기(gradient)**가 계산되어  \n",
    "> `.grad` 속성에 자동으로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd5c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2e3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0033, 0.4749, 0.2218],\n",
      "        [0.8056, 0.3840, 0.0867],\n",
      "        [0.5820, 0.8906, 0.5938],\n",
      "        [0.6829, 0.3664, 0.6069]], requires_grad=True) tensor([0.5548, 0.9362, 0.5315], requires_grad=True) tensor([2.6285, 3.0521, 2.0408], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x,W) + b\n",
    "print(W, b, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949683ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7964, grad_fn=<MseLossBackward0>) tensor([[1.7524, 2.0347, 1.3605],\n",
      "        [1.7524, 2.0347, 1.3605],\n",
      "        [1.7524, 2.0347, 1.3605],\n",
      "        [1.7524, 2.0347, 1.3605]]) tensor([1.7524, 2.0347, 1.3605])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(6.7964, grad_fn=<MseLossBackward0>) tensor([2.6285, 3.0521, 2.0408], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "2 tensor(3.0206, grad_fn=<MseLossBackward0>) tensor([1.7524, 2.0347, 1.3605], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "3 tensor(1.3425, grad_fn=<MseLossBackward0>) tensor([1.1682, 1.3565, 0.9070], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "4 tensor(0.5967, grad_fn=<MseLossBackward0>) tensor([0.7788, 0.9043, 0.6047], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "5 tensor(0.2652, grad_fn=<MseLossBackward0>) tensor([0.5192, 0.6029, 0.4031], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "6 tensor(0.1179, grad_fn=<MseLossBackward0>) tensor([0.3461, 0.4019, 0.2687], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "7 tensor(0.0524, grad_fn=<MseLossBackward0>) tensor([0.2308, 0.2679, 0.1792], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.1\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold :\n",
    "    iteration_num += 1\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "    \n",
    "    # detach_() : 텐서를 기존 방향성 비순환 그래프(DAG : Directed Acyclid Graph) 로부터 끊음\n",
    "    # .requires_grad(True) : 연결된 Tensor 로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듦\n",
    "    W.detach_().requires_grad_(True)\n",
    "    b.detach_().requires_grad_(True)\n",
    "\n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "\n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def7d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c87773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54fb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa92106b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
