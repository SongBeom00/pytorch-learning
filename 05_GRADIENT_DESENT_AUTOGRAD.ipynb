{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a617e3",
   "metadata": {},
   "source": [
    "### 자동 미분 (torch.autograd)\n",
    "\n",
    "- PyTorch의 autograd 는 신경망 훈련을 지원하는 자동 미분 가능\n",
    "- torch.autograd 동작 방법\n",
    "    - 텐서에 .requires_grad 속성을 True로 설정하면, 이후의 텐서 모든 연산들을 추적함\n",
    "    - 텐서.backward() 를 호출하면, 연산에 연결된 각 텐서들의 미분 값을 계산하여, 각 텐서 객체에 .grad에 저장\n",
    "        - .requires_grad_()는 연결된 Tensor로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5044f",
   "metadata": {},
   "source": [
    "### 신경망 동작 이해\n",
    "\n",
    "- 모델 및 데이터 생성\n",
    "- forward pass로 입력 데이터를 모델에 넣어서 예측값 계산\n",
    "- 예측값과 실제값의 차이를 loss function 으로 계산\n",
    "- backward pass 로 각 모델 파라미터를 loss 값 기반 미분하여 저장\n",
    "- optimizer 로 모델 파라미터의 최적값을 찾기 위해, 파라미터 값 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe383b7",
   "metadata": {},
   "source": [
    "- 텐서에 .requires_grad 속성을 True 로 설정\n",
    "- .requires_grad 속성이 True 로 설정되면, 텐서의 모든 연산 추적을 위해, 내부적으로 방향성 비순환 그래프(DAG : Directed Acyclic Graph)를 동적 구성\n",
    "    - 방향성 비순환 그래프(DAG)의 leaf 노드는 입력 텐서이고, root 노드는 결과 텐서가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "342f3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "y = torch.rand(1)\n",
    "y.requires_grad = True\n",
    "loss = y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfe2a7",
   "metadata": {},
   "source": [
    "### 🔁 텐서의 `.backward()` 동작 설명\n",
    "\n",
    "- `tensor.backward()` 를 호출하면,  \n",
    "  연산에 연결된 각 텐서들의 **미분 값(gradient)**을 자동으로 계산하여  \n",
    "  각 텐서 객체의 `.grad` 속성에 저장된다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 예시\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial x} = -1, \\quad \\frac{\\partial \\text{Loss}}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "> 즉, Loss를 기준으로 각 입력값에 대한 **기울기(gradient)**가 계산되어  \n",
    "> `.grad` 속성에 자동으로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcd5c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a2e3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5649, 0.3083, 0.0680],\n",
      "        [0.4738, 0.4295, 0.5688],\n",
      "        [0.9297, 0.4158, 0.8811],\n",
      "        [0.7368, 0.5742, 0.1030]], requires_grad=True) tensor([0.5654, 0.2019, 0.7719], requires_grad=True) tensor([3.2705, 1.9297, 2.3927], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x,W) + b\n",
    "print(W, b, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "949683ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7151, grad_fn=<MseLossBackward0>) tensor([[2.1804, 1.2864, 1.5952],\n",
      "        [2.1804, 1.2864, 1.5952],\n",
      "        [2.1804, 1.2864, 1.5952],\n",
      "        [2.1804, 1.2864, 1.5952]]) tensor([2.1804, 1.2864, 1.5952])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfb0109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(6.7151, grad_fn=<MseLossBackward0>) tensor([3.2705, 1.9297, 2.3927], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "2 tensor(2.9845, grad_fn=<MseLossBackward0>) tensor([2.1804, 1.2864, 1.5952], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "3 tensor(1.3264, grad_fn=<MseLossBackward0>) tensor([1.4536, 0.8576, 1.0634], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "4 tensor(0.5895, grad_fn=<MseLossBackward0>) tensor([0.9691, 0.5718, 0.7090], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "5 tensor(0.2620, grad_fn=<MseLossBackward0>) tensor([0.6460, 0.3812, 0.4726], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "6 tensor(0.1165, grad_fn=<MseLossBackward0>) tensor([0.4307, 0.2541, 0.3151], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "7 tensor(0.0518, grad_fn=<MseLossBackward0>) tensor([0.2871, 0.1694, 0.2101], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.1\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold :\n",
    "    iteration_num += 1\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "    \n",
    "    # detach_() : 텐서를 기존 방향성 비순환 그래프(DAG : Directed Acyclid Graph) 로부터 끊음\n",
    "    # .requires_grad(True) : 연결된 Tensor 로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듦\n",
    "    W.detach_().requires_grad_(True)\n",
    "    b.detach_().requires_grad_(True)\n",
    "\n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "\n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8e83c",
   "metadata": {},
   "source": [
    "### Optimizer 와 경사 하강법\n",
    "\n",
    "- 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해서 모델 매개변수를 조정하는 과정으로 Optimizer는 최적화 알고리즘을 의미합니다.\n",
    "- 대표적인 최적화 알고리즘에는 확률적 경사하강법(SGD: Stochastic Gradient Descent)\n",
    "- PyTorch 에는 모델과 데이터 타입에 따라, 보다 좋은 성능을 제공하는 ADAM 이나 RMSProp과 같은 다양한 옵티마이저가 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1def7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(4.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(4.0, requires_grad=True)\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)\n",
    "\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)\n",
    "\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b73491",
   "metadata": {},
   "source": [
    "### SGD 경사하강법 Optimizer 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6c87773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a993df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD([W, b], lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54fb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300 tensor([[0.3289, 0.2782, 0.7777],\n",
      "        [0.6289, 0.9512, 0.1976],\n",
      "        [0.8176, 0.2167, 0.3775],\n",
      "        [0.8384, 0.0892, 0.3002]], requires_grad=True) tensor([0.9355, 0.1868, 0.7305], requires_grad=True) tensor(7.5781, grad_fn=<MseLossBackward0>)\n",
      "100 300 tensor([[-0.3571, -0.0546,  0.3171],\n",
      "        [-0.0570,  0.6184, -0.2630],\n",
      "        [ 0.1316, -0.1161, -0.0832],\n",
      "        [ 0.1525, -0.2437, -0.1604]], requires_grad=True) tensor([ 0.2496, -0.1460,  0.2698], requires_grad=True) tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "200 300 tensor([[-0.3802, -0.0658,  0.3016],\n",
      "        [-0.0801,  0.6072, -0.2785],\n",
      "        [ 0.1085, -0.1273, -0.0987],\n",
      "        [ 0.1294, -0.2549, -0.1759]], requires_grad=True) tensor([ 0.2265, -0.1573,  0.2543], requires_grad=True) tensor(9.7781e-06, grad_fn=<MseLossBackward0>)\n",
      "300 300 tensor([[-0.3810, -0.0662,  0.3010],\n",
      "        [-0.0809,  0.6068, -0.2791],\n",
      "        [ 0.1078, -0.1277, -0.0992],\n",
      "        [ 0.1286, -0.2552, -0.1765]], requires_grad=True) tensor([ 0.2257, -0.1576,  0.2538], requires_grad=True) tensor(1.1093e-08, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 300  # 원하는 만큼 경사하강법 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    \n",
    "    optimizer.zero_grad() # 기울기 초기화\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 파라미터 업데이트 \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, nb_epochs, W, b, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12905801",
   "metadata": {},
   "source": [
    "### SGD 경사하강법 Optimizer 적용 (PyTorch 신경망 모델 클래스 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e4f705c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel(4,3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61b9b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    pred = model(x)\n",
    "    loss = F.mse_loss(pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f72ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5065e-13, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2709, -0.2626,  0.2644, -0.3309],\n",
      "        [ 0.5202,  0.0626, -0.3250, -0.3387],\n",
      "        [-0.2581, -0.2793,  0.4233,  0.1462]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0583,  0.0808, -0.0321], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
