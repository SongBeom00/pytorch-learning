{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a617e3",
   "metadata": {},
   "source": [
    "### ìë™ ë¯¸ë¶„ (torch.autograd)\n",
    "\n",
    "- PyTorchì˜ autograd ëŠ” ì‹ ê²½ë§ í›ˆë ¨ì„ ì§€ì›í•˜ëŠ” ìë™ ë¯¸ë¶„ ê°€ëŠ¥\n",
    "- torch.autograd ë™ì‘ ë°©ë²•\n",
    "    - í…ì„œì— .requires_grad ì†ì„±ì„ Trueë¡œ ì„¤ì •í•˜ë©´, ì´í›„ì˜ í…ì„œ ëª¨ë“  ì—°ì‚°ë“¤ì„ ì¶”ì í•¨\n",
    "    - í…ì„œ.backward() ë¥¼ í˜¸ì¶œí•˜ë©´, ì—°ì‚°ì— ì—°ê²°ëœ ê° í…ì„œë“¤ì˜ ë¯¸ë¶„ ê°’ì„ ê³„ì‚°í•˜ì—¬, ê° í…ì„œ ê°ì²´ì— .gradì— ì €ì¥\n",
    "        - .requires_grad_()ëŠ” ì—°ê²°ëœ Tensorë¡œë¶€í„°ì˜ ê³„ì‚°ëœ ìë™ë¯¸ë¶„ ê°’ì„, ë‹¤ì‹œ í˜„ í…ì„œë¶€í„° ì‹œì‘í•˜ë„ë¡ ë§Œë“¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5044f",
   "metadata": {},
   "source": [
    "### ì‹ ê²½ë§ ë™ì‘ ì´í•´\n",
    "\n",
    "- ëª¨ë¸ ë° ë°ì´í„° ìƒì„±\n",
    "- forward passë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ì„œ ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
    "- ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ loss function ìœ¼ë¡œ ê³„ì‚°\n",
    "- backward pass ë¡œ ê° ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ loss ê°’ ê¸°ë°˜ ë¯¸ë¶„í•˜ì—¬ ì €ì¥\n",
    "- optimizer ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´, íŒŒë¼ë¯¸í„° ê°’ ì—…ë°ì´íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe383b7",
   "metadata": {},
   "source": [
    "- í…ì„œì— .requires_grad ì†ì„±ì„ True ë¡œ ì„¤ì •\n",
    "- .requires_grad ì†ì„±ì´ True ë¡œ ì„¤ì •ë˜ë©´, í…ì„œì˜ ëª¨ë“  ì—°ì‚° ì¶”ì ì„ ìœ„í•´, ë‚´ë¶€ì ìœ¼ë¡œ ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG : Directed Acyclic Graph)ë¥¼ ë™ì  êµ¬ì„±\n",
    "    - ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG)ì˜ leaf ë…¸ë“œëŠ” ì…ë ¥ í…ì„œì´ê³ , root ë…¸ë“œëŠ” ê²°ê³¼ í…ì„œê°€ ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "342f3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "y = torch.rand(1)\n",
    "y.requires_grad = True\n",
    "loss = y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfe2a7",
   "metadata": {},
   "source": [
    "### ğŸ” í…ì„œì˜ `.backward()` ë™ì‘ ì„¤ëª…\n",
    "\n",
    "- `tensor.backward()` ë¥¼ í˜¸ì¶œí•˜ë©´,  \n",
    "  ì—°ì‚°ì— ì—°ê²°ëœ ê° í…ì„œë“¤ì˜ **ë¯¸ë¶„ ê°’(gradient)**ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬  \n",
    "  ê° í…ì„œ ê°ì²´ì˜ `.grad` ì†ì„±ì— ì €ì¥ëœë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ ì˜ˆì‹œ\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial x} = -1, \\quad \\frac{\\partial \\text{Loss}}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "> ì¦‰, Lossë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ì…ë ¥ê°’ì— ëŒ€í•œ **ê¸°ìš¸ê¸°(gradient)**ê°€ ê³„ì‚°ë˜ì–´  \n",
    "> `.grad` ì†ì„±ì— ìë™ìœ¼ë¡œ ì €ì¥ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcd5c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a2e3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5649, 0.3083, 0.0680],\n",
      "        [0.4738, 0.4295, 0.5688],\n",
      "        [0.9297, 0.4158, 0.8811],\n",
      "        [0.7368, 0.5742, 0.1030]], requires_grad=True) tensor([0.5654, 0.2019, 0.7719], requires_grad=True) tensor([3.2705, 1.9297, 2.3927], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x,W) + b\n",
    "print(W, b, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "949683ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7151, grad_fn=<MseLossBackward0>) tensor([[2.1804, 1.2864, 1.5952],\n",
      "        [2.1804, 1.2864, 1.5952],\n",
      "        [2.1804, 1.2864, 1.5952],\n",
      "        [2.1804, 1.2864, 1.5952]]) tensor([2.1804, 1.2864, 1.5952])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfb0109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(6.7151, grad_fn=<MseLossBackward0>) tensor([3.2705, 1.9297, 2.3927], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "2 tensor(2.9845, grad_fn=<MseLossBackward0>) tensor([2.1804, 1.2864, 1.5952], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "3 tensor(1.3264, grad_fn=<MseLossBackward0>) tensor([1.4536, 0.8576, 1.0634], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "4 tensor(0.5895, grad_fn=<MseLossBackward0>) tensor([0.9691, 0.5718, 0.7090], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "5 tensor(0.2620, grad_fn=<MseLossBackward0>) tensor([0.6460, 0.3812, 0.4726], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "6 tensor(0.1165, grad_fn=<MseLossBackward0>) tensor([0.4307, 0.2541, 0.3151], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "7 tensor(0.0518, grad_fn=<MseLossBackward0>) tensor([0.2871, 0.1694, 0.2101], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.1\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold :\n",
    "    iteration_num += 1\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "    \n",
    "    # detach_() : í…ì„œë¥¼ ê¸°ì¡´ ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG : Directed Acyclid Graph) ë¡œë¶€í„° ëŠìŒ\n",
    "    # .requires_grad(True) : ì—°ê²°ëœ Tensor ë¡œë¶€í„°ì˜ ê³„ì‚°ëœ ìë™ë¯¸ë¶„ ê°’ì„, ë‹¤ì‹œ í˜„ í…ì„œë¶€í„° ì‹œì‘í•˜ë„ë¡ ë§Œë“¦\n",
    "    W.detach_().requires_grad_(True)\n",
    "    b.detach_().requires_grad_(True)\n",
    "\n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "\n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8e83c",
   "metadata": {},
   "source": [
    "### Optimizer ì™€ ê²½ì‚¬ í•˜ê°•ë²•\n",
    "\n",
    "- ìµœì í™”ëŠ” ê° í•™ìŠµ ë‹¨ê³„ì—ì„œ ëª¨ë¸ì˜ ì˜¤ë¥˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ OptimizerëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "- ëŒ€í‘œì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì—ëŠ” í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD: Stochastic Gradient Descent)\n",
    "- PyTorch ì—ëŠ” ëª¨ë¸ê³¼ ë°ì´í„° íƒ€ì…ì— ë”°ë¼, ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” ADAM ì´ë‚˜ RMSPropê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1def7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(4.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(4.0, requires_grad=True)\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)\n",
    "\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)\n",
    "\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b73491",
   "metadata": {},
   "source": [
    "### SGD ê²½ì‚¬í•˜ê°•ë²• Optimizer ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6c87773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a993df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD([W, b], lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54fb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300 tensor([[0.3289, 0.2782, 0.7777],\n",
      "        [0.6289, 0.9512, 0.1976],\n",
      "        [0.8176, 0.2167, 0.3775],\n",
      "        [0.8384, 0.0892, 0.3002]], requires_grad=True) tensor([0.9355, 0.1868, 0.7305], requires_grad=True) tensor(7.5781, grad_fn=<MseLossBackward0>)\n",
      "100 300 tensor([[-0.3571, -0.0546,  0.3171],\n",
      "        [-0.0570,  0.6184, -0.2630],\n",
      "        [ 0.1316, -0.1161, -0.0832],\n",
      "        [ 0.1525, -0.2437, -0.1604]], requires_grad=True) tensor([ 0.2496, -0.1460,  0.2698], requires_grad=True) tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "200 300 tensor([[-0.3802, -0.0658,  0.3016],\n",
      "        [-0.0801,  0.6072, -0.2785],\n",
      "        [ 0.1085, -0.1273, -0.0987],\n",
      "        [ 0.1294, -0.2549, -0.1759]], requires_grad=True) tensor([ 0.2265, -0.1573,  0.2543], requires_grad=True) tensor(9.7781e-06, grad_fn=<MseLossBackward0>)\n",
      "300 300 tensor([[-0.3810, -0.0662,  0.3010],\n",
      "        [-0.0809,  0.6068, -0.2791],\n",
      "        [ 0.1078, -0.1277, -0.0992],\n",
      "        [ 0.1286, -0.2552, -0.1765]], requires_grad=True) tensor([ 0.2257, -0.1576,  0.2538], requires_grad=True) tensor(1.1093e-08, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 300  # ì›í•˜ëŠ” ë§Œí¼ ê²½ì‚¬í•˜ê°•ë²• ë°˜ë³µ\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    \n",
    "    optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "    loss.backward() # ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "    optimizer.step() # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, nb_epochs, W, b, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12905801",
   "metadata": {},
   "source": [
    "### SGD ê²½ì‚¬í•˜ê°•ë²• Optimizer ì ìš© (PyTorch ì‹ ê²½ë§ ëª¨ë¸ í´ë˜ìŠ¤ ê¸°ë°˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e4f705c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel(4,3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61b9b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    pred = model(x)\n",
    "    loss = F.mse_loss(pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f72ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5065e-13, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2709, -0.2626,  0.2644, -0.3309],\n",
      "        [ 0.5202,  0.0626, -0.3250, -0.3387],\n",
      "        [-0.2581, -0.2793,  0.4233,  0.1462]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0583,  0.0808, -0.0321], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
