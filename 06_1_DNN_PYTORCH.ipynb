{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b3fd04",
   "metadata": {},
   "source": [
    "### 활성화 함수\n",
    "- __init__() 함수에서는 모델에서 사용될 모듈(nn.Linear 등)과 activation function(활성화 함수) 등을 정의함\n",
    "- forward() 함수에서 실행되어야 하는 연산에 활성화 함수도 적용하면 됨\n",
    "- 주요 활성화 함수\n",
    "    - 시그모이드 함수 : nn.Sigmoid()\n",
    "    - ReLU 함수 : nn.ReLU()\n",
    "    - Leaky ReLU 함수 : nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fae421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim, out_features=output_dim)\n",
    "        self.activation = nn.Sigmoid() # 시그모이드 함수\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c03b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99f7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f250db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0243, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.7735, -0.2206, -0.0008,  0.1168],\n",
      "        [-0.6442, -0.4289, -0.5716,  0.1211],\n",
      "        [-0.3785, -0.3623, -0.7004,  0.0541]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.7881, -0.3031, -0.2112], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095e1fc",
   "metadata": {},
   "source": [
    "### 다층 레이어 구현\n",
    "> raw level 로 구현해본 후, 좀더 유용한 클래스를 알아보기로\n",
    "\n",
    "- input layer -> hidden layer -> output layer 순으로 순차적으로 작성해주면 됨\n",
    "    - 내부 행렬곱 조건만 유의해주면 됨\n",
    "\n",
    "- activation function 적용은 output layer 에는 적용하지 않는 것이 일반적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc057743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 10)\n",
    "        self.linear2 = nn.Linear(10, 10)\n",
    "        self.linear3 = nn.Linear(10, 10)\n",
    "        self.linear4 = nn.Linear(10, output_dim)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim, output_dim)\n",
    "        hidden = self.activation(self.linear1(x)) # |hidden| = (input_dim, 5)\n",
    "        hidden = self.activation(self.linear2(hidden)) # |hidden| = (5, 5)\n",
    "        hidden = self.activation(self.linear3(hidden)) # |hidden| = (5, 5)\n",
    "        y = self.linear4(hidden) # 마지막 출력에는 activation 함수를 사용하지 않는 것이 일반적임\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f88056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427d85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63049483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3889e-11, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0727,  0.0699, -0.4880, -0.4512],\n",
      "        [ 0.3895,  0.2990,  0.2938,  0.0815],\n",
      "        [ 0.2743, -0.0419, -0.1630, -0.4318],\n",
      "        [-0.3889,  0.2126, -0.2284, -0.2245],\n",
      "        [ 0.0524,  0.2113,  0.2521,  0.3501],\n",
      "        [ 0.0673,  0.2327, -0.2678,  0.4528],\n",
      "        [-0.2693,  0.3649,  0.4550,  0.3656],\n",
      "        [ 0.1441,  0.1624, -0.4174,  0.2337],\n",
      "        [ 0.2957,  0.3314,  0.4524,  0.4067],\n",
      "        [ 0.0877, -0.3260,  0.1033,  0.4192]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2755,  0.3230,  0.4459,  0.2198,  0.3354,  0.4236, -0.4003,  0.2015,\n",
      "         0.0520,  0.1049], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2284, -0.2767, -0.0242,  0.1591,  0.1088, -0.0907, -0.0245,  0.0584,\n",
      "         -0.0551,  0.0116],\n",
      "        [-0.0637, -0.1881,  0.2396,  0.2462,  0.0616,  0.1611,  0.0751,  0.2162,\n",
      "         -0.2784,  0.2664],\n",
      "        [ 0.0400,  0.2052, -0.1707, -0.1115,  0.2480, -0.1998,  0.0567, -0.1424,\n",
      "          0.2673, -0.2486],\n",
      "        [-0.0460, -0.0315, -0.0759, -0.2097, -0.2523,  0.0785,  0.0292, -0.1814,\n",
      "         -0.2721, -0.2970],\n",
      "        [-0.0105, -0.3156,  0.2976,  0.1111, -0.1289, -0.2959,  0.2149,  0.0462,\n",
      "          0.3086, -0.2914],\n",
      "        [-0.2059, -0.2579,  0.0029, -0.2925,  0.2315, -0.0063, -0.1642, -0.2166,\n",
      "          0.0607,  0.2551],\n",
      "        [-0.2088,  0.0791,  0.0719,  0.2959, -0.3357,  0.2205,  0.2944,  0.0449,\n",
      "          0.0625,  0.2881],\n",
      "        [-0.0025,  0.2164, -0.0964, -0.2585,  0.2209,  0.1787, -0.2610, -0.1130,\n",
      "         -0.0571,  0.1200],\n",
      "        [ 0.0992, -0.0843, -0.0636, -0.0911, -0.0074,  0.0768, -0.1960,  0.0367,\n",
      "         -0.1527, -0.3122],\n",
      "        [ 0.2006,  0.2187,  0.1884,  0.3079, -0.1168, -0.1392,  0.0717,  0.0149,\n",
      "         -0.1491, -0.1159]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2784,  0.0406,  0.3134,  0.2958, -0.1765,  0.0080, -0.0337, -0.3085,\n",
      "        -0.0661,  0.1764], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2567, -0.1704,  0.1317,  0.0862, -0.0546, -0.0250,  0.2796, -0.1415,\n",
      "         -0.2601,  0.2380],\n",
      "        [-0.0749, -0.1903,  0.1031, -0.0285, -0.2686, -0.2948, -0.2788,  0.1983,\n",
      "          0.1327, -0.2523],\n",
      "        [ 0.0828, -0.0067,  0.3098, -0.2844,  0.1391, -0.3008,  0.0347, -0.0221,\n",
      "          0.0148,  0.0037],\n",
      "        [-0.0205,  0.0417, -0.0093,  0.0436,  0.2748,  0.1501, -0.0394,  0.2841,\n",
      "          0.2275,  0.1922],\n",
      "        [ 0.1856, -0.0945, -0.2993,  0.2201, -0.1194, -0.1292, -0.1161,  0.1002,\n",
      "          0.2203,  0.1987],\n",
      "        [ 0.0385,  0.2490, -0.0336,  0.1481,  0.2904, -0.3032, -0.2506,  0.3056,\n",
      "         -0.1347, -0.1776],\n",
      "        [ 0.2351,  0.1368, -0.2931, -0.0844,  0.1432, -0.0313, -0.0812, -0.2258,\n",
      "          0.1532,  0.2666],\n",
      "        [-0.1520, -0.0688,  0.1189, -0.2768,  0.1401,  0.0499, -0.2453, -0.1865,\n",
      "          0.0493, -0.1147],\n",
      "        [ 0.1318,  0.0693, -0.1171,  0.2490, -0.3081, -0.1787,  0.0228,  0.2766,\n",
      "         -0.0184,  0.0710],\n",
      "        [ 0.2477,  0.2520, -0.2624,  0.2587, -0.3075, -0.2485,  0.1568,  0.2208,\n",
      "          0.1263, -0.1334]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0781, -0.0616,  0.1336, -0.0100, -0.2987, -0.1665,  0.3044, -0.1512,\n",
      "        -0.3072, -0.1816], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1835,  0.1248,  0.2568, -0.2109, -0.2654, -0.0547,  0.2673,  0.1502,\n",
      "         -0.1053, -0.1521],\n",
      "        [-0.0363,  0.2255, -0.2456, -0.0502, -0.0234,  0.1390,  0.3082,  0.2737,\n",
      "         -0.2936,  0.0330],\n",
      "        [ 0.1487, -0.1720, -0.2015,  0.2225,  0.2948, -0.2778, -0.2775,  0.0593,\n",
      "         -0.0487, -0.0271]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0910,  0.1118,  0.0618], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845325e",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "\n",
    "- nn.Sequential 은 순서를 갖는 모듈의 컨테이너를 의미함\n",
    "- 순차적으로 연산되는 레이어만 있을 경우에는, nn.Sequential을 통해 순서대로 각 레이어를 작성하면 그대로 실행됨\n",
    "    - 중간에 activation function이 적용된다면, activation function도 순서에 맞게 넣어주면 자동 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8a3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(x.size(0))\n",
    "print(y.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9f0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.size(0)\n",
    "output_dim = y.size(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, output_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7efff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4144e-11, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.0442,  0.2914,  0.3071,  0.3162],\n",
      "        [-0.1451,  0.3318, -0.1291, -0.3782],\n",
      "        [-0.0738, -0.2778, -0.3890, -0.4934],\n",
      "        [-0.0363, -0.1630, -0.1278,  0.0555],\n",
      "        [-0.4286, -0.1900,  0.0995, -0.2950],\n",
      "        [ 0.4898, -0.0046,  0.2333,  0.4315],\n",
      "        [-0.3636, -0.2509,  0.1210, -0.2384],\n",
      "        [-0.0533,  0.3700, -0.3006, -0.2195],\n",
      "        [-0.1318,  0.0449, -0.0167,  0.2088],\n",
      "        [-0.1904, -0.0896, -0.0158, -0.0880]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4280,  0.0463,  0.3380, -0.0250,  0.1573,  0.3354,  0.4536, -0.3715,\n",
      "         0.0501,  0.2163], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.1600e-01,  8.2027e-02,  2.1091e-02, -4.7452e-02,  5.3687e-02,\n",
      "          1.5900e-01,  1.9247e-01, -4.3328e-02, -3.1011e-01, -2.7504e-02],\n",
      "        [-8.9423e-02,  3.6057e-02, -3.9426e-02,  2.4418e-01,  1.9936e-01,\n",
      "         -2.4272e-02,  1.3459e-01,  9.1155e-02,  7.9613e-02, -2.4531e-01],\n",
      "        [-1.0797e-01, -1.5125e-01,  2.8828e-01,  2.2994e-01, -3.0148e-01,\n",
      "         -2.0476e-01, -5.9476e-02, -2.3376e-01, -9.1361e-02, -6.8110e-03],\n",
      "        [-1.9564e-01,  1.2397e-01,  2.0106e-01, -1.4603e-01, -1.9475e-01,\n",
      "          1.1731e-01, -5.1362e-02, -6.9551e-02, -1.0977e-01,  1.7136e-02],\n",
      "        [ 4.7383e-02,  2.7651e-01, -2.4443e-01,  1.2271e-01, -7.1773e-02,\n",
      "          6.5431e-02,  3.9803e-02, -8.9000e-03,  2.4105e-01,  3.1772e-02],\n",
      "        [-1.9860e-01,  7.6169e-02, -1.7273e-01,  1.0482e-01, -2.3493e-02,\n",
      "          6.4133e-02, -2.2641e-01, -6.2355e-02,  9.6963e-02, -2.3582e-01],\n",
      "        [ 1.9358e-01,  2.2453e-01,  1.7088e-01, -9.9126e-02, -2.5602e-01,\n",
      "         -1.0979e-01, -1.2429e-01,  1.9967e-02, -8.6544e-02, -1.7317e-01],\n",
      "        [-2.2418e-01,  2.6329e-01,  1.1469e-04, -2.8319e-01,  2.7259e-01,\n",
      "         -1.6502e-01,  2.2071e-01,  8.0444e-02, -2.9229e-02,  1.7908e-02],\n",
      "        [ 2.3555e-01,  1.7524e-02, -2.1319e-01, -6.7792e-02, -4.9847e-02,\n",
      "          1.0689e-01, -2.8111e-01,  2.1023e-01,  7.1601e-02, -3.6222e-02],\n",
      "        [ 2.4068e-01, -1.7029e-01, -1.3423e-01,  2.1123e-01, -2.5519e-01,\n",
      "         -2.7464e-01, -2.9154e-01,  4.5528e-03, -2.5482e-01,  2.0991e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0025,  0.2371, -0.0855,  0.0914, -0.3154, -0.2756, -0.2507,  0.2770,\n",
      "         0.0205,  0.1930], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659,  0.1487, -0.2797,  0.0480, -0.0446, -0.0463,  0.1754,  0.0128,\n",
      "          0.0867, -0.0425],\n",
      "        [ 0.2551, -0.1529,  0.1302, -0.2567,  0.0336, -0.1313,  0.0809, -0.0860,\n",
      "         -0.2968, -0.2737],\n",
      "        [ 0.2468, -0.0690, -0.2570,  0.0835, -0.2779, -0.2444, -0.2826, -0.2243,\n",
      "         -0.1287, -0.1384],\n",
      "        [-0.2211,  0.1575, -0.2179,  0.3039,  0.1907,  0.0710, -0.0967, -0.3144,\n",
      "         -0.2194, -0.2190],\n",
      "        [ 0.0468,  0.1620,  0.1853,  0.2212,  0.2981,  0.2487,  0.0956, -0.2436,\n",
      "          0.0676, -0.0649],\n",
      "        [ 0.1769,  0.1146,  0.0832, -0.2149,  0.1803, -0.2955,  0.3126, -0.0567,\n",
      "         -0.0303,  0.2603],\n",
      "        [ 0.2406,  0.0431, -0.2485,  0.0186, -0.0852,  0.2839,  0.1600, -0.2224,\n",
      "         -0.0030,  0.1612],\n",
      "        [-0.1528, -0.2794,  0.0968, -0.1473,  0.2364,  0.2462, -0.1621, -0.0612,\n",
      "          0.0085,  0.0989],\n",
      "        [ 0.2320,  0.1345,  0.1641, -0.0673,  0.1417, -0.1844, -0.1330,  0.2506,\n",
      "          0.0041, -0.2616],\n",
      "        [-0.1936,  0.0450,  0.0952, -0.0767, -0.2834,  0.2888, -0.1970,  0.1300,\n",
      "          0.2613,  0.2098]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2520,  0.0998,  0.0739,  0.1701, -0.2476, -0.1304,  0.1282, -0.1567,\n",
      "         0.1864,  0.0176], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1544,  0.0311, -0.1387,  0.1965,  0.0737, -0.2969, -0.1606, -0.2930,\n",
      "          0.1008,  0.1422],\n",
      "        [-0.1314,  0.2005, -0.2835, -0.1696, -0.2887, -0.1642,  0.1826,  0.3180,\n",
      "          0.1430,  0.0719],\n",
      "        [-0.2850,  0.2085, -0.1882,  0.0844, -0.1016, -0.1372,  0.1610,  0.1246,\n",
      "          0.1509, -0.1258]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0309, 0.0304, 0.0442], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb548d",
   "metadata": {},
   "source": [
    "### SGD 방식 구현\n",
    "\n",
    "- 랜덤하게 데이터를 섞기 위한 함수\n",
    "    - torch.randperm(n): 0 ~ n -1 까지의 정수를 랜덤하게 섞어서, 순열(배열)을 만들어줌\n",
    "    - torch.index_select(텐서객체, 차원번호, 인덱스텐서)\n",
    "        - 차원번호는 예를 들어 , $|x|$ = (3, 4) 에서 0차원에 해당하는 값은 3 (행), 1차원에 해당하는 값은 4(열)\n",
    "    - 특정 차원의 나열된 인덱스 번호 순서대로, 데이터를 섞어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e837823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4876,  0.8673,  0.7370, -1.2567],\n",
      "        [ 0.2807,  0.5982, -1.6942,  0.1102],\n",
      "        [ 3.0521, -1.9946, -1.5569,  1.0603]])\n",
      "tensor([1, 2])\n",
      "tensor([[ 0.2807,  0.5982, -1.6942,  0.1102],\n",
      "        [ 3.0521, -1.9946, -1.5569,  1.0603]])\n",
      "tensor([[ 0.8673,  0.7370],\n",
      "        [ 0.5982, -1.6942],\n",
      "        [-1.9946, -1.5569]])\n"
     ]
    }
   ],
   "source": [
    "data1 = torch.randn(3, 4)\n",
    "print(data1)\n",
    "indices = torch.tensor([1, 2])\n",
    "print(indices)\n",
    "print(torch.index_select(data1, 0, indices)) # 행을 기준으로\n",
    "print(torch.index_select(data1, 1, indices)) # 열을 기준으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4fc3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5000, 10) # 10개의 feature 가 있는 5000개의 데이터셋\n",
    "y = torch.zeros(5000, 1) # \n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "minibatch_size = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.size(-1) # 입력 차원 10\n",
    "output_dim = y.size(-1) # 출력 차원 1\n",
    "\n",
    "# 보통 hidden layer는 출력에 가까울 수록 작아지게 설계하는 것이 일반적임(더 좋은 성능)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 8),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(8, 6),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(6, output_dim)\n",
    ")\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3708f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4620,  519, 2458,  ..., 4110, 3361, 4613])\n",
      "torch.Size([5000, 10]) torch.Size([5000, 1])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(x.size(0)) # 5000개의 인덱스 번호를 랜덤하게 섞는다.\n",
    "print(indices)\n",
    "x_batch_list = torch.index_select(x, 0, indices) # shuffle 된 데이터셋으로, 데이터양이 상당하므로, 미니배치 변수로 선언\n",
    "y_batch_list = torch.index_select(y, 0, indices) # shuffle 된 데이터셋으로, 데이터양이 상당하므로, 미니배치 변수로 선언\n",
    "\n",
    "print(x_batch_list.shape, y_batch_list.shape) # 데이터를 섞기만 해서 shape는 똑같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    }
   ],
   "source": [
    "x_batch_list = x_batch_list.split(minibatch_size, dim=0) # dim=0 으로 행을 기준으로 5000 /256 = 20\n",
    "y_batch_list = y_batch_list.split(minibatch_size, dim=0) \n",
    "print(len(x_batch_list), len(y_batch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382e95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([256, 10])\n"
     ]
    }
   ],
   "source": [
    "print(type(x_batch_list)) # split 으로 나누어서 return 은 tuple이 반환됨\n",
    "print(type(x_batch_list[0]))\n",
    "print(x_batch_list[0].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6ce04",
   "metadata": {},
   "source": [
    "#### row 레벨로 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e5c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.9936e-15, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.0922,  0.1433,  0.1727, -0.0115, -0.1301, -0.0221, -0.2143, -0.0926,\n",
      "         -0.0563,  0.2415],\n",
      "        [-0.2077, -0.0885, -0.1740,  0.0632, -0.0773, -0.2268, -0.2123,  0.2850,\n",
      "          0.0751,  0.0347],\n",
      "        [-0.1581,  0.2713,  0.0846, -0.0986,  0.0255,  0.1926,  0.2736, -0.3051,\n",
      "         -0.2049, -0.1968],\n",
      "        [ 0.0322, -0.1635, -0.1822, -0.1106,  0.2086, -0.0444, -0.1952,  0.1646,\n",
      "         -0.1709,  0.2675],\n",
      "        [ 0.1097, -0.2520,  0.2383, -0.0055, -0.0331, -0.0695, -0.2775, -0.1034,\n",
      "          0.1557, -0.3065],\n",
      "        [-0.0510,  0.2764,  0.2185,  0.0529, -0.0005,  0.0539, -0.0418, -0.2067,\n",
      "          0.0677,  0.2719],\n",
      "        [ 0.2058,  0.1629,  0.1587, -0.0511, -0.0157,  0.0961,  0.2607,  0.0938,\n",
      "         -0.2570, -0.0088],\n",
      "        [ 0.1108,  0.2921, -0.1645, -0.2396,  0.2485, -0.1816, -0.2722, -0.2914,\n",
      "          0.2913, -0.2421],\n",
      "        [ 0.2525, -0.3134,  0.0926,  0.1103,  0.2756, -0.1086,  0.0676, -0.1020,\n",
      "          0.1067, -0.2223],\n",
      "        [-0.2650, -0.1324,  0.2872,  0.1806,  0.1760,  0.0622,  0.1888,  0.2401,\n",
      "          0.2098,  0.0978]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2423, -0.0408, -0.0416, -0.3041,  0.1356, -0.2463, -0.1105,  0.1434,\n",
      "        -0.0860, -0.1498], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1339,  0.2025, -0.1110,  0.3037,  0.1122, -0.0143, -0.1596,  0.2792,\n",
      "         -0.2720,  0.1792],\n",
      "        [-0.0063, -0.2141,  0.0563, -0.0183, -0.0259, -0.0732,  0.0566,  0.0227,\n",
      "          0.2991, -0.0655],\n",
      "        [ 0.2504,  0.2168, -0.3023,  0.0634, -0.1436, -0.1482,  0.0127, -0.2540,\n",
      "         -0.1840,  0.0940],\n",
      "        [ 0.2254, -0.0510, -0.2119,  0.3018, -0.1511, -0.2370,  0.1744,  0.2754,\n",
      "         -0.2551, -0.0116],\n",
      "        [ 0.1061, -0.2286, -0.1317, -0.1105, -0.1285, -0.2881,  0.1633,  0.2080,\n",
      "          0.1829,  0.1234],\n",
      "        [ 0.0067,  0.0709,  0.1503,  0.0600, -0.1882,  0.2749,  0.2744, -0.3043,\n",
      "          0.1334, -0.2168],\n",
      "        [-0.2438,  0.0062, -0.1998, -0.1449, -0.1977,  0.0404,  0.1406, -0.1400,\n",
      "          0.2913,  0.2287],\n",
      "        [-0.2155,  0.0749,  0.2738, -0.2019, -0.1682,  0.0852, -0.3050, -0.3006,\n",
      "          0.1571,  0.0272]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1792,  0.1872, -0.1799,  0.1462, -0.0806,  0.0309,  0.0276, -0.2102],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.5356e-01,  1.6268e-01, -2.5996e-01,  1.3911e-01,  1.4035e-01,\n",
      "          3.3364e-01,  1.5241e-01, -1.3204e-04],\n",
      "        [-1.4589e-01,  8.6335e-02, -4.2773e-02, -3.0128e-01,  4.6792e-02,\n",
      "          2.3167e-01,  1.1854e-01, -2.7930e-01],\n",
      "        [ 2.2890e-01,  3.3210e-01, -3.0746e-01, -3.1157e-01, -1.9034e-01,\n",
      "          3.4160e-02, -2.7219e-02, -1.8497e-02],\n",
      "        [ 4.9593e-02,  3.6458e-02, -1.5415e-01, -5.9625e-02, -2.2288e-01,\n",
      "         -3.0666e-01,  4.4796e-02, -1.1840e-02],\n",
      "        [ 2.7111e-01,  1.2064e-01,  2.9427e-02,  6.5813e-02,  2.3567e-01,\n",
      "         -3.4998e-01,  7.9932e-02, -1.7220e-01],\n",
      "        [-2.6454e-01, -4.1975e-02,  2.1441e-01,  1.5489e-01,  3.1804e-02,\n",
      "          2.1171e-01,  9.2612e-02,  2.0381e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3143,  0.0626,  0.1934,  0.0273,  0.0698,  0.2892],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2971,  0.2329, -0.0091, -0.3087, -0.0796, -0.3145]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1006], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for index in range(nb_epochs):\n",
    "    indices = torch.randperm(x.size(0))\n",
    "    \n",
    "    x_batch_list = torch.index_select(x, dim=0, index=indices) # 5000개를 무작위로 shuffle \n",
    "    y_batch_list = torch.index_select(y, dim=0, index=indices) # 5000개를 무작위로 shuffle \n",
    "    x_batch_list = x_batch_list.split(minibatch_size, dim=0) # 행을 기준으로 minibatch_size 만큼 나눈다.\n",
    "    y_batch_list = y_batch_list.split(minibatch_size, dim=0) # 행을 기준으로 minibatch_size 만큼 나눈다.\n",
    "    \n",
    "    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        loss = loss_function(y_minibatch_pred, y_minibatch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "print(loss)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73683b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
