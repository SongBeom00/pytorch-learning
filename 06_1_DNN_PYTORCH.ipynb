{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b3fd04",
   "metadata": {},
   "source": [
    "### 활성화 함수\n",
    "- __init__() 함수에서는 모델에서 사용될 모듈(nn.Linear 등)과 activation function(활성화 함수) 등을 정의함\n",
    "- forward() 함수에서 실행되어야 하는 연산에 활성화 함수도 적용하면 됨\n",
    "- 주요 활성화 함수\n",
    "    - 시그모이드 함수 : nn.Sigmoid()\n",
    "    - ReLU 함수 : nn.ReLU()\n",
    "    - Leaky ReLU 함수 : nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9fae421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim, out_features=output_dim)\n",
    "        self.activation = nn.Sigmoid() # 시그모이드 함수\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99f7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f250db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.4241, -0.0435, -0.2485, -0.6506],\n",
      "        [-0.1905, -0.5900,  0.1035, -0.6944],\n",
      "        [-0.5265, -0.2848, -0.6738,  0.0304]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5693, -0.4355, -0.2349], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095e1fc",
   "metadata": {},
   "source": [
    "### 다층 레이어 구현\n",
    "> raw level 로 구현해본 후, 좀더 유용한 클래스를 알아보기로\n",
    "\n",
    "- input layer -> hidden layer -> output layer 순으로 순차적으로 작성해주면 됨\n",
    "    - 내부 행렬곱 조건만 유의해주면 됨\n",
    "\n",
    "- activation function 적용은 output layer 에는 적용하지 않는 것이 일반적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc057743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 10)\n",
    "        self.linear2 = nn.Linear(10, 10)\n",
    "        self.linear3 = nn.Linear(10, 10)\n",
    "        self.linear4 = nn.Linear(10, output_dim)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (input_dim, output_dim)\n",
    "        hidden = self.activation(self.linear1(x)) # |hidden| = (input_dim, 5)\n",
    "        hidden = self.activation(self.linear2(hidden)) # |hidden| = (5, 5)\n",
    "        hidden = self.activation(self.linear3(hidden)) # |hidden| = (5, 5)\n",
    "        y = self.linear4(hidden) # 마지막 출력에는 activation 함수를 사용하지 않는 것이 일반적임\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f88056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427d85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63049483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.9183e-13, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.2594,  0.2687, -0.1575,  0.0258],\n",
      "        [-0.0046,  0.1706,  0.0414, -0.2126],\n",
      "        [-0.4967, -0.3701,  0.3450, -0.2120],\n",
      "        [ 0.2302, -0.0048,  0.0419, -0.0267],\n",
      "        [ 0.0569, -0.1314, -0.2341, -0.0464],\n",
      "        [-0.0558, -0.1660, -0.2447,  0.1743],\n",
      "        [-0.3571, -0.0055,  0.3971, -0.2283],\n",
      "        [-0.0469, -0.1339,  0.3037,  0.0028],\n",
      "        [ 0.5048,  0.2312,  0.0802, -0.2120],\n",
      "        [ 0.0446, -0.2198,  0.2883,  0.1191]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2643, -0.2484, -0.2908, -0.0925, -0.2556, -0.4381,  0.0411, -0.0194,\n",
      "        -0.4093, -0.2954], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2516, -0.1534, -0.1862, -0.0030,  0.2118,  0.3057,  0.0363,  0.1584,\n",
      "          0.2502, -0.2606],\n",
      "        [ 0.2126, -0.1156,  0.0283,  0.0137,  0.0626, -0.0864,  0.0269,  0.2058,\n",
      "          0.2150,  0.2660],\n",
      "        [-0.0787, -0.0700,  0.1254, -0.0005, -0.0781,  0.1856, -0.1857, -0.2947,\n",
      "         -0.0701,  0.1193],\n",
      "        [-0.2027,  0.2412,  0.1744,  0.1669,  0.2483,  0.3037,  0.2246,  0.2948,\n",
      "          0.2787,  0.1589],\n",
      "        [ 0.1213,  0.2382, -0.2306, -0.1157,  0.0914,  0.0154,  0.2529,  0.1953,\n",
      "          0.2423,  0.2942],\n",
      "        [ 0.1358,  0.1048,  0.0838,  0.2148, -0.1473, -0.3116,  0.1994, -0.1433,\n",
      "          0.2805,  0.1498],\n",
      "        [-0.0542, -0.0619, -0.2176, -0.1331, -0.0701, -0.1308,  0.2201, -0.1833,\n",
      "          0.1540,  0.1618],\n",
      "        [-0.2734, -0.2240, -0.3044, -0.2344,  0.2177, -0.1830,  0.1965, -0.1472,\n",
      "          0.1593,  0.0197],\n",
      "        [-0.0855,  0.3138,  0.0497,  0.0266,  0.0525,  0.0456,  0.2970, -0.2554,\n",
      "          0.2790, -0.1529],\n",
      "        [ 0.0829, -0.2769, -0.2205,  0.2167, -0.0470,  0.0495,  0.2586,  0.1903,\n",
      "         -0.2206, -0.2809]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2893, -0.0198, -0.2213, -0.2580, -0.2192,  0.0574,  0.0149,  0.1019,\n",
      "        -0.1531,  0.1967], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0161,  0.2291,  0.0500,  0.0183, -0.1295,  0.1469,  0.1048,  0.2991,\n",
      "          0.1371,  0.1362],\n",
      "        [ 0.2374, -0.0629,  0.2937, -0.1830,  0.3067, -0.1973,  0.2621, -0.2724,\n",
      "         -0.0243,  0.2756],\n",
      "        [-0.1961, -0.1725, -0.1389,  0.0919, -0.2898,  0.1075, -0.0287,  0.3106,\n",
      "         -0.1320, -0.0004],\n",
      "        [ 0.0396, -0.1132,  0.3113,  0.1184, -0.1417,  0.1017,  0.0129,  0.2513,\n",
      "          0.1960,  0.0371],\n",
      "        [ 0.3039, -0.0451,  0.2047,  0.1896, -0.0886,  0.0744,  0.2134,  0.2082,\n",
      "          0.3043,  0.2739],\n",
      "        [ 0.0448, -0.0399,  0.0216, -0.2609, -0.0369, -0.2405,  0.0130, -0.0172,\n",
      "         -0.1269, -0.0150],\n",
      "        [-0.0504,  0.1834,  0.2687,  0.0094, -0.2072,  0.1638, -0.1816,  0.0489,\n",
      "          0.1580,  0.0959],\n",
      "        [ 0.1018,  0.1215,  0.2195,  0.2022, -0.1446, -0.2912,  0.1553,  0.0256,\n",
      "          0.2671,  0.2656],\n",
      "        [ 0.1231, -0.1494, -0.0426, -0.1898,  0.1218, -0.2543,  0.1344, -0.2668,\n",
      "          0.2936,  0.2783],\n",
      "        [ 0.0955, -0.2865,  0.2140,  0.2416, -0.0006, -0.1879, -0.0516,  0.2590,\n",
      "         -0.2154,  0.2649]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0122,  0.2592,  0.2743,  0.0659,  0.3118, -0.1482, -0.0970,  0.3383,\n",
      "         0.0194, -0.1679], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0085,  0.2070, -0.0693, -0.1693, -0.3055, -0.3122, -0.2724, -0.2890,\n",
      "          0.2344, -0.0488],\n",
      "        [ 0.1804,  0.1330, -0.3057,  0.0557, -0.2316,  0.0581, -0.0868,  0.1218,\n",
      "         -0.1856, -0.1146],\n",
      "        [ 0.0517, -0.0426, -0.1654, -0.1940,  0.1511,  0.1184,  0.0523,  0.0077,\n",
      "          0.1860,  0.0436]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1925, 0.0891, 0.0146], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845325e",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "\n",
    "- nn.Sequential 은 순서를 갖는 모듈의 컨테이너를 의미함\n",
    "- 순차적으로 연산되는 레이어만 있을 경우에는, nn.Sequential을 통해 순서대로 각 레이어를 작성하면 그대로 실행됨\n",
    "    - 중간에 activation function이 적용된다면, activation function도 순서에 맞게 넣어주면 자동 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b8a3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(x.size(0))\n",
    "print(y.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce9f0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.size(0)\n",
    "output_dim = y.size(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, output_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c7efff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7523e-10, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.2444,  0.4977, -0.4331, -0.2907],\n",
      "        [ 0.0037,  0.2101,  0.0058, -0.4933],\n",
      "        [-0.2397,  0.3261, -0.3702, -0.1141],\n",
      "        [-0.4896,  0.0995,  0.3092,  0.4234],\n",
      "        [ 0.4409,  0.2386, -0.1198, -0.3903],\n",
      "        [ 0.1417,  0.1793,  0.4791, -0.1199],\n",
      "        [ 0.0262,  0.4756, -0.3681,  0.0800],\n",
      "        [ 0.0783, -0.4238, -0.0289,  0.4479],\n",
      "        [ 0.2518,  0.0613,  0.4202,  0.1782],\n",
      "        [-0.4930, -0.4162,  0.4472,  0.3580]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0204, -0.3141, -0.4616,  0.2718, -0.3736,  0.2561, -0.0242, -0.4492,\n",
      "        -0.2618,  0.0959], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.0888e-01,  3.5507e-02,  1.3827e-01, -1.6195e-01,  2.1074e-01,\n",
      "         -6.4053e-02, -7.2814e-02,  2.1355e-01, -6.8119e-02, -8.8605e-02],\n",
      "        [ 2.5369e-01, -1.2928e-01, -3.9691e-02, -1.5260e-01, -1.8764e-01,\n",
      "          1.5276e-01, -5.6802e-02, -2.3592e-01,  6.0384e-02, -3.3532e-02],\n",
      "        [-1.1602e-04, -2.7253e-02, -1.6542e-01,  5.9764e-02, -1.4398e-01,\n",
      "         -2.6981e-01, -1.4865e-01,  1.7201e-01,  1.7963e-01,  5.2882e-02],\n",
      "        [-1.1105e-01,  2.1582e-01,  3.0055e-01,  1.2666e-01, -2.8800e-01,\n",
      "         -1.0073e-01,  2.3724e-01,  6.1655e-02, -1.1508e-01,  1.8343e-01],\n",
      "        [-1.6673e-01, -2.9449e-01,  1.8394e-01, -6.8059e-02, -1.2035e-01,\n",
      "         -3.0060e-01, -1.0437e-01,  2.8300e-01,  2.4981e-01,  5.8586e-02],\n",
      "        [ 2.6176e-02,  2.8168e-01,  1.5704e-01, -4.9811e-02,  2.3120e-01,\n",
      "          2.5490e-01, -4.6644e-02,  6.2977e-02,  1.5938e-01, -2.2445e-01],\n",
      "        [-1.7924e-02,  2.8139e-01, -7.7213e-03,  3.1580e-01, -6.4130e-02,\n",
      "          2.6087e-01,  3.0398e-01, -1.9224e-01, -1.8334e-01,  1.0506e-01],\n",
      "        [-2.6584e-01,  1.7646e-01,  1.1010e-01,  1.2731e-01, -2.5869e-01,\n",
      "         -1.6631e-01,  2.0730e-01,  1.5336e-01, -2.2511e-01, -2.2571e-01],\n",
      "        [-2.3092e-01, -5.8575e-02,  3.6189e-02, -2.2196e-01,  2.6378e-01,\n",
      "          1.8023e-01,  8.7611e-02, -5.8324e-02,  3.0561e-01, -1.9530e-01],\n",
      "        [ 2.6032e-01,  2.4315e-01,  1.6570e-01, -2.7451e-01,  2.2899e-01,\n",
      "          3.1257e-02,  2.3466e-01, -2.1414e-01, -1.3270e-01, -9.4349e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1021,  0.0777, -0.1054, -0.3038, -0.0468,  0.0045,  0.0275, -0.0655,\n",
      "         0.0625,  0.3029], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1643,  0.1870,  0.2969,  0.3053, -0.2122,  0.0597, -0.1697, -0.1130,\n",
      "          0.0109,  0.0490],\n",
      "        [ 0.2505,  0.1523,  0.0215,  0.0366, -0.1701, -0.0992, -0.2866, -0.2402,\n",
      "         -0.1939, -0.3114],\n",
      "        [ 0.1099,  0.0180,  0.1671, -0.1101, -0.1297,  0.0239, -0.0169,  0.2060,\n",
      "         -0.1442, -0.0667],\n",
      "        [ 0.0370, -0.1636,  0.0970,  0.0571,  0.0114, -0.3008, -0.1357,  0.2205,\n",
      "          0.0312, -0.2205],\n",
      "        [-0.2108,  0.1228,  0.0128, -0.3002,  0.0107,  0.2305, -0.2555, -0.0933,\n",
      "          0.2337, -0.1056],\n",
      "        [ 0.2574, -0.2691, -0.1775,  0.2020,  0.0848, -0.0214,  0.3171, -0.0597,\n",
      "         -0.1083, -0.2784],\n",
      "        [ 0.2476, -0.0537, -0.3027, -0.1261, -0.2593, -0.2270,  0.1629, -0.0525,\n",
      "          0.2429, -0.2420],\n",
      "        [-0.1475,  0.1146,  0.1073, -0.0813,  0.2617, -0.0782,  0.0114,  0.2677,\n",
      "          0.0498, -0.2786],\n",
      "        [ 0.0384,  0.2117,  0.2777, -0.2515, -0.1148,  0.1578, -0.0048, -0.0670,\n",
      "         -0.2608,  0.2637],\n",
      "        [ 0.3005, -0.1481,  0.2348,  0.2763, -0.1515, -0.2033, -0.3119,  0.0478,\n",
      "          0.2928, -0.0375]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0575, -0.2920,  0.1020, -0.2122, -0.1372,  0.2264, -0.3104, -0.2752,\n",
      "        -0.0071,  0.1121], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1153,  0.0887,  0.0351,  0.2687, -0.2046, -0.1546,  0.1188,  0.2026,\n",
      "          0.1724,  0.1869],\n",
      "        [-0.2011,  0.0241, -0.2721, -0.1073,  0.0304,  0.2284,  0.1112, -0.1683,\n",
      "          0.0392, -0.1387],\n",
      "        [-0.2429, -0.2955, -0.3044,  0.0570,  0.0873,  0.0800, -0.3113,  0.2974,\n",
      "         -0.0129,  0.2167]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0554, -0.0478, -0.0156], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
